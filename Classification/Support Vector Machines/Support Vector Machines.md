# SVM 

our goal is to maximize the margin. <br> The margin is defined as the distance between the seperating hyperplane (decision boundary) <br> 
the training examples that are closest to this hyperplane are called *support vectors* <br> 
![image](https://github.com/user-attachments/assets/75503e38-2b99-4a7d-8925-36e4d686d2ec)

## slack variable æ¾å¼›å˜é‡
soft margin classification <br> 
the linear constraints in the SVM optimization objective need to be relaxed for nonlinearly seperable data to allow convergence of the optimization in the presence of misclassifications, under appropriate loss penalization <br> 
ä»¥ä¾¿åœ¨æ ·æœ¬å¹¶éå®Œå…¨çº¿æ€§å¯åˆ†çš„æƒ…å†µä¸‹ä»èƒ½æ”¶æ•›ï¼Œå¹¶ä¸”åœ¨æœ‰åˆ†ç±»é”™è¯¯çš„å‰æä¸‹ï¼Œä¼˜åŒ–å™¨å¯ä»¥å®¹å¿ä¸€å®šçš„è¯¯å·® <br> 

![image](https://github.com/user-attachments/assets/cef13db4-c19d-48f0-86d1-3c75ef71fc31)

c ä¸ºL2 æ­£åˆ™åŒ–çš„å€’æ•° <br> 


##  Why Linear Models Fail on Some Data Distributions

Some data distributions are **nonlinear in nature**, which means they cannot be separated using a straight line (linear hyperplane). For example:

###  XOR Problem

- (0, 0) and (1, 1) belong to one class  
- (1, 0) and (0, 1) belong to the other class  
-  You **cannot** draw a single straight line to separate these points

###  Concentric Circles

- One class is in the inner circle, the other class is in the outer circle  
-  You **cannot** use a single straight line to separate the inner and outer rings

---

These kinds of data are **linearly inseparable in 2D space**, meaning:
> A straight line will never be a good decision boundary.

This is why we need **nonlinear models** or **kernel methods** to transform or map the data into a higher-dimensional space where a linear separator becomes possible.

##  Kernel Methods: Making Nonlinear Data Linearly Separable

To deal with linearly inseparable data, we use **kernel methods**.  
The key idea is to **construct nonlinear combinations of the original features**, projecting the data into a higher-dimensional space.

> This is done using a **mapping function** \( \phi \), where the data may become linearly separable in the new space.

As shown in *Figure 3.14*, we can transform a 2D dataset into a new 3D feature space, where the classes become separable by projecting with the following transformation:

```math
\phi(x_1, x_2) = (z_1, z_2, z_3) = (x_1, x_2, x_1^2 + x_2^2)
```

![image](https://github.com/user-attachments/assets/d5a4b84f-5843-4f3a-a690-9c06402503d7)

To avoid explicitly computing the dot product between two points in a high-dimensional space,  
we define a **kernel function** as follows:

```math
\kappa(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) = \phi(\mathbf{x}^{(i)})^T \phi(\mathbf{x}^{(j)})
```


## RBF Kernel (Radial Basis Function Kernel)

One of the most commonly used kernels is the **Radial Basis Function (RBF)** kernel,  
also known as the **Gaussian kernel**:

```math
\kappa(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) = \exp\left( -\frac{ \| \mathbf{x}^{(i)} - \mathbf{x}^{(j)} \|^2 }{2\sigma^2} \right)
```

### Simplified RBF Kernel

The RBF kernel is often simplified to:

```math
\kappa(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) = \exp\left( -\gamma \| \mathbf{x}^{(i)} - \mathbf{x}^{(j)} \|^2 \right)
```

Where:

```math
\gamma = \frac{1}{2\sigma^2}
```
is a hyperparameter that must be tuned. <br> 


![image](https://github.com/user-attachments/assets/fb2c8ba6-8430-4dcc-848f-f32675274a26)


##  `gamma` åœ¨ SVM ä¸­çš„å«ä¹‰

å½“ä½ ä½¿ç”¨ **æ ¸å‡½æ•°ï¼ˆkernel functionï¼‰**ï¼Œæ¯”å¦‚ RBF æ ¸ï¼ˆä¹Ÿå« Gaussian æ ¸ï¼‰æ—¶ï¼Œ`gamma` æ§åˆ¶çš„æ˜¯ï¼š

> æ¯ä¸ªæ ·æœ¬å¯¹æœ€ç»ˆæ¨¡å‹çš„â€œå½±å“èŒƒå›´â€æœ‰å¤šå¤§ã€‚

å®ƒå®é™…ä¸Šæ˜¯æ ¸å‡½æ•°ä¸­çš„ä¸€ä¸ªè¶…å‚æ•°ï¼Œå‡ºç°åœ¨ä¸‹é¢è¿™ä¸ªå…¬å¼ä¸­ï¼š

$$
\kappa(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) = \exp\left( -\gamma \|\mathbf{x}^{(i)} - \mathbf{x}^{(j)}\|^2 \right)
$$


###  å¤§ gammaï¼š**æ¯ä¸ªç‚¹åªå½±å“è‡ªå·±å‘¨å›´çš„å°åŒºåŸŸ**
- **ç»“æœ**ï¼šæ¨¡å‹ä¼šè®°ä½è®­ç»ƒæ•°æ®ï¼Œå½¢æˆéå¸¸èœ¿èœ’ã€**å¤æ‚çš„è¾¹ç•Œ**ã€‚
- **ä¼˜ç‚¹**ï¼šèƒ½é«˜åº¦æ‹Ÿåˆè®­ç»ƒé›†ã€‚
- **ç¼ºç‚¹**ï¼šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¹æ–°æ ·æœ¬æ³›åŒ–èƒ½åŠ›å·®ã€‚

###  å° gammaï¼š**æ¯ä¸ªç‚¹çš„å½±å“èŒƒå›´å¾ˆå¤§**
- **ç»“æœ**ï¼šæ¨¡å‹æ›´å…³æ³¨å…¨å±€ç»“æ„ï¼Œå½¢æˆå¹³æ»‘ã€**ç®€å•çš„è¾¹ç•Œ**ã€‚
- **ä¼˜ç‚¹**ï¼šæ³›åŒ–èƒ½åŠ›å¥½ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
- **ç¼ºç‚¹**ï¼šå®¹æ˜“æ¬ æ‹Ÿåˆï¼Œå¿½ç•¥å±€éƒ¨ç»†èŠ‚ã€‚

---

## ğŸ¨ å¯è§†åŒ–ç±»æ¯”

- å¤§ gamma â‰ˆ æ”¾å¤§é•œçœ‹æ¯ä¸ªç‚¹ï¼Œçœ¼é‡Œåªæœ‰é™„è¿‘  
- å° gamma â‰ˆ ä»è¿œå¤„çœ‹æ•´ä½“åˆ†å¸ƒï¼Œç»†èŠ‚æ¨¡ç³Šä½†èƒ½çœ‹åˆ°è¶‹åŠ¿
